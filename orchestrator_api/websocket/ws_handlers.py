from flask_socketio import SocketIO, emit
from flask import request
from eventlet import tpool
import requests
import tempfile
import os
import base64
import uuid
from app import socketio
from services.answer_service import (
    stream_text_answer_by_sentence,
    synthesize_wav,
    transcribe_audio
)
import structlog
from dotenv import load_dotenv, find_dotenv
from services.object_extraction_service import extract_prompts
from queue import Queue
from services.rag_singleton import rag_manager

# Gets the logger instance
logger = structlog.get_logger()
active_pdf_by_sid = {}

load_dotenv(find_dotenv())
TEXT_TO_IMAGE_URL = os.getenv("ZIMAGE_TURBO_API_URL")
IMAGE_TO_3D_MODEL = os.getenv("IMAGE_TO_3D_MODEL")
if IMAGE_TO_3D_MODEL == "Trellis":
    IMAGE_TO_3D_URL = os.getenv("TRELLIS_API_URL")
else:
    IMAGE_TO_3D_URL = os.getenv("HUNYUAN_API_URL")

@socketio.on('connect')
def on_connect():
    logger.info(f"[WS] CONNECT sid={request.sid}")

@socketio.on('disconnect')
def on_disconnect():
    logger.info(f"[WS] DISCONNECT sid={request.sid}")
    active_pdf_by_sid.pop(request.sid, None)


@socketio.on("ask")
def handle_ask(data):
    """
    WebSocket event handler.
    Expects:
      - 'audio' (base64), mandatory
      - 'audio_response' (bool), optional
      - 'objects' (int: 0, 1), optional
      - 'max_objects' (int), optional
    """
    # Generate a unique ID for this specific interaction
    rid = str(uuid.uuid4())
    structlog.contextvars.clear_contextvars()
    structlog.contextvars.bind_contextvars(rid=rid, connection="websocket")
    
    try:
        sid = request.sid
        pdf_name = active_pdf_by_sid.get(sid)
        if not pdf_name:
            emit("error", {"message": "No PDF selected. Please select a PDF first."}, to=sid)
            return

        st, err = rag_manager.get_status(pdf_name)
        if st != "ready":
            emit("error", {"message": f"PDF not ready: {st}. Please wait."}, to=sid)
            return

        audio_response = data.get("audio_response", True)
        object_gen = int(data.get("objects", 0))
        audio_b64 = data.get("audio_question")
        audio_bytes = base64.b64decode(audio_b64)
        max_objects = data.get("max_objects", 1)
        
        # Save incoming temporary audio
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(audio_bytes)
            tmp_path = tmp.name

        # STT
        transcription, language = transcribe_audio(tmp_path)

        emit("language_detected", {"language": language})

        # 1. Create a Queue for audio tasks
        audio_queue = Queue()

        # 2. Define the Audio Worker function
        def audio_worker(sid):
            while True:
                text_to_speak = audio_queue.get()
                if text_to_speak is None: # Exit signal
                    socketio.emit("audio_done", {"status": "completed"}, to=sid)
                    break

                try:
                    wav_b64 = tpool.execute(synthesize_wav, text_to_speak, language=language)
                    
                    socketio.emit("audio_sentence", {"base64": wav_b64}, to=sid)
                except Exception as e:
                    logger.error("audio_synthesis_failed", error=str(e))
                
                # Allow other tasks (like summary emission) to jump in
                socketio.sleep(0)
                audio_queue.task_done()

        # 3. Start the worker thread
        socketio.start_background_task(audio_worker, sid)

        try:
            # Buffer to collect the full text answer (optional, depending on if you want summary in logs)
            full_answer = []
            
            # Variables to accumulate the summary
            summary_title = ""
            summary_bullets = []

            # For each item generated by the LLM:
            # Note: sentence_tuple is now (text_content, message_type)
            for text_content, message_type in stream_text_answer_by_sentence(transcription, pdf_name, sid):

                if message_type == "speech":
                    # Handle Speech Text
                    # Send text chunk to main chat window
                    emit("text_chunk", {"text": text_content})
                    full_answer.append(text_content)

                    # Push to audio queue for background processing
                    if audio_response:
                        audio_queue.put(text_content)

                elif message_type == "title":
                    # Capture the title
                    summary_title = text_content

                elif message_type == "bullet":
                    # Capture the bullet point
                    summary_bullets.append(text_content)
                
                socketio.sleep(0)

            # Stream Finished: Emit the Whole Summary Event
            # We only emit this once, containing the full structure
            if summary_title or summary_bullets:
                emit("summary", {
                    "title": summary_title,
                    "body": summary_bullets
                })

            # Logging
            answer_text = " ".join(full_answer)
            logger.info(f"Tutor response: {answer_text}")
            emit("text_done", {"status": "completed"})

        finally:
            # Clean up: Tell the worker to stop after the queue is empty
            audio_queue.put(None) 

        obj_extracted, summary_img_extracted = extract_prompts(transcription, answer_text)

        # Retrieve the current rid from the context
        context = structlog.contextvars.get_contextvars()
        rid = context.get("rid")
        # Define common headers to pass the rid to the other microservices
        headers = {"X-Request-ID": rid} if rid else {}

        try:
            txt2img_response = requests.post(
                TEXT_TO_IMAGE_URL, 
                json={
                    "prompt": summary_img_extracted.prompt,
                    "summary": "true"
                }, 
                headers=headers,
                timeout=20
            )
            # Check status before accessing .json()
            if txt2img_response.status_code == 200:
                emit("summary_image", {
                    "image_id": txt2img_response.json().get("image_id"),
                    "caption": summary_img_extracted.caption
                })
            else:
                logger.error("txt2img_error", status=txt2img_response.status_code)

        except requests.exceptions.RequestException as e:
            logger.error("txt2img_connection_failed", error=str(e))


        # 3D object generation
        if object_gen:
            socketio.start_background_task(
                run_object_generation, sid, obj_extracted, language, headers
            )

    except Exception as e:
        emit("error", {"message": str(e)})

# Generate 3D models and send their filenames to the client
def run_object_generation(sid, obj_extracted, language, headers):

    try:
        socketio.sleep(0)
        obj_presentation_audio = synthesize_wav(obj_extracted.speech, language)

        # --- STEP 1: Text to Image ---
        try:
            txt2img_response = requests.post(
                TEXT_TO_IMAGE_URL, 
                json={
                    "prompt": obj_extracted.prompt,
                    "summary": False
                }, 
                headers=headers,
                timeout=20
            )
        except requests.exceptions.RequestException as e:
            logger.error("txt2img_connection_failed", error=str(e))

        if txt2img_response.status_code == 200:
            img_id = txt2img_response.json().get("image_id")

            # --- STEP 2: Image to 3D ---
            try:
                img2obj_response = requests.post(
                    IMAGE_TO_3D_URL, 
                    json={"img_id": img_id}, 
                    headers=headers,
                    timeout=120
                )
            except requests.exceptions.RequestException as e:
                logger.error("img2obj_connection_failed", error=str(e))

            if img2obj_response.status_code == 200:
                obj_filename = img2obj_response.json().get("object_id")
                socketio.emit("object", {
                    "object": obj_filename, 
                    "text": obj_extracted.speech,
                    "speech": obj_presentation_audio
                }, to=sid)
            else:
                logger.error("img2obj_error_status", status=img2obj_response.status_code)
        else:
            logger.error("txt2img_error_status", status=txt2img_response.status_code)

    except Exception as e:
        logger.error("generation_pipeline_crashed", error=str(e))

    socketio.emit("objects_done", {"status": "completed"}, to=sid)


@socketio.on("pdf_selected")
def handle_pdf_selected(data):
    sid = request.sid
    pdf_name = (data or {}).get("pdf_name", "").strip().lower()

    if not pdf_name:
        emit("pdf_selected_ack", {"status": "error", "message": "missing_pdf_name"}, to=sid)
        return

    active_pdf_by_sid[sid] = pdf_name

    st, err = rag_manager.get_status(pdf_name)
    if st == "ready":
        emit("pdf_selected_ack", {"pdf_name": pdf_name, "status": "ok"}, to=sid)
        return
    if st == "building":
        # opzionale
        emit("pdf_selected_ack", {"pdf_name": pdf_name, "status": "building"}, to=sid)
        return

    def build_and_ack():
        ok, msg = tpool.execute(rag_manager.ensure_ready, pdf_name)
        if ok:
            socketio.emit("pdf_selected_ack", {"pdf_name": pdf_name, "status": "ok"}, to=sid)
        else:
            st2, err2 = rag_manager.get_status(pdf_name)
            socketio.emit(
                "pdf_selected_ack",
                {"pdf_name": pdf_name, "status": "error", "message": err2 or msg},
                to=sid
            )

    socketio.start_background_task(build_and_ack)

